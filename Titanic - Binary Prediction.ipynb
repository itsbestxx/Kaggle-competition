{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-18T09:52:26.615724Z","iopub.execute_input":"2022-07-18T09:52:26.616239Z","iopub.status.idle":"2022-07-18T09:52:26.656566Z","shell.execute_reply.started":"2022-07-18T09:52:26.616126Z","shell.execute_reply":"2022-07-18T09:52:26.655883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:26.658607Z","iopub.execute_input":"2022-07-18T09:52:26.659246Z","iopub.status.idle":"2022-07-18T09:52:29.051077Z","shell.execute_reply.started":"2022-07-18T09:52:26.659203Z","shell.execute_reply":"2022-07-18T09:52:29.050090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_classif as mi\nfrom sklearn import metrics\nfrom pprint import pprint\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nimport lightgbm as gbm","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:29.054940Z","iopub.execute_input":"2022-07-18T09:52:29.055195Z","iopub.status.idle":"2022-07-18T09:52:29.526421Z","shell.execute_reply.started":"2022-07-18T09:52:29.055167Z","shell.execute_reply":"2022-07-18T09:52:29.525548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:06.898635Z","iopub.execute_input":"2022-07-18T09:54:06.899155Z","iopub.status.idle":"2022-07-18T09:54:06.904482Z","shell.execute_reply.started":"2022-07-18T09:54:06.899114Z","shell.execute_reply":"2022-07-18T09:54:06.903859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:29.527721Z","iopub.execute_input":"2022-07-18T09:52:29.527969Z","iopub.status.idle":"2022-07-18T09:52:29.554284Z","shell.execute_reply.started":"2022-07-18T09:52:29.527939Z","shell.execute_reply":"2022-07-18T09:52:29.553335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot\n\ninit_notebook_mode(connected=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:29.556184Z","iopub.execute_input":"2022-07-18T09:52:29.556439Z","iopub.status.idle":"2022-07-18T09:52:30.860133Z","shell.execute_reply.started":"2022-07-18T09:52:29.556410Z","shell.execute_reply":"2022-07-18T09:52:30.859209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, HTML","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:30.861205Z","iopub.execute_input":"2022-07-18T09:52:30.861408Z","iopub.status.idle":"2022-07-18T09:52:30.866234Z","shell.execute_reply.started":"2022-07-18T09:52:30.861383Z","shell.execute_reply":"2022-07-18T09:52:30.865222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Analysis**\nLet's first explore the data","metadata":{}},{"cell_type":"code","source":"dfs = pd.concat([train_data, test_data], ignore_index = True)\ndfs.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:30.867743Z","iopub.execute_input":"2022-07-18T09:52:30.868076Z","iopub.status.idle":"2022-07-18T09:52:30.910904Z","shell.execute_reply.started":"2022-07-18T09:52:30.868034Z","shell.execute_reply":"2022-07-18T09:52:30.910172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp.ProfileReport(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:30.912524Z","iopub.execute_input":"2022-07-18T09:52:30.913045Z","iopub.status.idle":"2022-07-18T09:52:30.917035Z","shell.execute_reply.started":"2022-07-18T09:52:30.912991Z","shell.execute_reply":"2022-07-18T09:52:30.916389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trace_fare = go.Histogram(x = dfs.Fare)\nfig = make_subplots(rows = 2, cols = 3, vertical_spacing = 0.1, horizontal_spacing = 0.1,\n                   subplot_titles = ('Fare Distribution', 'Age Distribution',\n                                     'SibSp Count', 'Parch Count', 'Pclass Count',\n                                    'Embarked Count'))\n\nfig.add_trace(go.Histogram(x = dfs.Fare), row = 1, col = 1)\nfig.add_trace(go.Histogram(x = dfs.Age), row = 1, col = 2)\nfig.add_trace(go.Bar(x = dfs.SibSp.value_counts().index,\n                     y = dfs.SibSp.value_counts()), row = 1, col = 3)\nfig.add_trace(go.Bar(x = dfs.Parch.value_counts().index,\n                    y = dfs.Parch.value_counts()), row = 2, col = 1)\nfig.add_trace(go.Bar(x = dfs.Pclass.value_counts().index,\n                    y = dfs.Pclass.value_counts()), row = 2, col =2)\nfig.add_trace(go.Bar(x = dfs.Embarked.value_counts().index,\n                    y = dfs.Embarked.value_counts()), row = 2, col =3)\n\n\nfig.update_layout(bargap = 0.2, height = 750, showlegend = False)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:30.917884Z","iopub.execute_input":"2022-07-18T09:52:30.918079Z","iopub.status.idle":"2022-07-18T09:52:31.231584Z","shell.execute_reply.started":"2022-07-18T09:52:30.918055Z","shell.execute_reply":"2022-07-18T09:52:31.230689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's examine the correlation of each variable. But we need to first convert sex column into int32 and text/object datatype to dummy variable","metadata":{}},{"cell_type":"code","source":"#Convert Sex column into binary format\ngender = {'male':1, 'female':0}\ndfs.Sex = [gender[sex] for sex in dfs.Sex]\n\n#Convert Embarked to dummy variables\ndef create_dummy(data, feature):\n    features = []\n    features.append(feature)\n    data = pd.get_dummies(data, columns = features)\n    return data\n\n#Retaining Embark column\nembark_holder = dfs['Embarked']\ndfs = create_dummy(dfs, 'Embarked')\ndfs['Embarked'] = embark_holder","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:31.233257Z","iopub.execute_input":"2022-07-18T09:52:31.233842Z","iopub.status.idle":"2022-07-18T09:52:31.250061Z","shell.execute_reply.started":"2022-07-18T09:52:31.233798Z","shell.execute_reply":"2022-07-18T09:52:31.249441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.imshow(dfs.corr(), aspect = 'auto', origin = 'lower',\n                color_continuous_scale = 'Rdbu_r')\n\nanno = dfs.corr().values\n\nfor i, r in enumerate(anno):\n    for k, j in enumerate(r):\n        fig.add_annotation(x=k, y=i, text = \"{:0.3f}\".format(j), showarrow = False)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:31.252254Z","iopub.execute_input":"2022-07-18T09:52:31.252987Z","iopub.status.idle":"2022-07-18T09:52:33.315702Z","shell.execute_reply.started":"2022-07-18T09:52:31.252935Z","shell.execute_reply":"2022-07-18T09:52:33.314781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sex and Pclass seems to have relatively high impact on Survived","metadata":{}},{"cell_type":"markdown","source":"# Missing Data Imputation\n## Age\nThere are many missing data in Age variable. Let's first determine how we will go around this. It might be tempting to just fill missing age with median or mean but this is not a good idea for high variance variable or if we have a lot of missing data. In this case, we can use information from other columns to infer Age. For starter, we can use linear regression model to predict age from highly correlated variable. Also the Title of the person's name can also infer Age (eg. Master = male child = under 15)","metadata":{}},{"cell_type":"code","source":"dfs['Title'] = dfs.Name.str.extract('([A-Za-z]+)\\.', expand = False)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.316892Z","iopub.execute_input":"2022-07-18T09:52:33.317729Z","iopub.status.idle":"2022-07-18T09:52:33.325118Z","shell.execute_reply.started":"2022-07-18T09:52:33.317692Z","shell.execute_reply":"2022-07-18T09:52:33.324244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that while there is a 'Master' title for male child, there is none for female child. So we will have to manually assign 'FemaleChild' title to those with 'Miss' title with age under 12, and for those who have age missing, we will assign it to those that has Parch > 0 and groupsize > 1","metadata":{}},{"cell_type":"code","source":"#Make new column of the size of people with the same ticket number\ndfs['GroupSize'] = dfs.Ticket.map(dfs.Ticket.value_counts())\ndfs['famsize'] = dfs['Parch'] + dfs['SibSp'] + 1","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.326078Z","iopub.execute_input":"2022-07-18T09:52:33.326301Z","iopub.status.idle":"2022-07-18T09:52:33.340306Z","shell.execute_reply.started":"2022-07-18T09:52:33.326273Z","shell.execute_reply":"2022-07-18T09:52:33.339605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs.loc[(dfs['Title'] == 'Miss') & (dfs['Parch'] > 0) & (dfs['GroupSize'] > 1) & (dfs['Age'] <= 12), 'Title'] = 'FemaleChild'","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.341825Z","iopub.execute_input":"2022-07-18T09:52:33.342697Z","iopub.status.idle":"2022-07-18T09:52:33.349517Z","shell.execute_reply.started":"2022-07-18T09:52:33.342659Z","shell.execute_reply":"2022-07-18T09:52:33.348692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs.loc[(dfs['Title'] == 'Miss') & (dfs['Parch'] > 0) & (dfs['GroupSize'] > 1) & (dfs['Pclass'] == 3) & (dfs.Age.isnull()), 'Title'] = 'FemaleChild'","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.350790Z","iopub.execute_input":"2022-07-18T09:52:33.351737Z","iopub.status.idle":"2022-07-18T09:52:33.365662Z","shell.execute_reply.started":"2022-07-18T09:52:33.351701Z","shell.execute_reply":"2022-07-18T09:52:33.364789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use linear regression to predict Age using these correlated variable","metadata":{}},{"cell_type":"code","source":"dfs_td = create_dummy(dfs, 'Title')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.366910Z","iopub.execute_input":"2022-07-18T09:52:33.367164Z","iopub.status.idle":"2022-07-18T09:52:33.380877Z","shell.execute_reply.started":"2022-07-18T09:52:33.367134Z","shell.execute_reply":"2022-07-18T09:52:33.379858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_features = ['Pclass','SibSp', 'Sex', 'Parch']\nfor col in dfs_td.columns:\n    if 'Title' in col:\n        model_features.append(col)\ndf_nona = dfs_td[dfs_td['Age'].notna()]\ny_true = df_nona['Age']\nmodel = LinearRegression()\nmodel.fit(X = df_nona[model_features], y = y_true)\ny_pred = model.predict(df_nona[model_features])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.382826Z","iopub.execute_input":"2022-07-18T09:52:33.383428Z","iopub.status.idle":"2022-07-18T09:52:33.417919Z","shell.execute_reply.started":"2022-07-18T09:52:33.383382Z","shell.execute_reply":"2022-07-18T09:52:33.416917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate MAE, MSE, RMSE\nprint(metrics.mean_absolute_error(y_true, y_pred))\nprint(metrics.mean_squared_error(y_true, y_pred))\nprint(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\nresiduals = y_true - y_pred","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.424142Z","iopub.execute_input":"2022-07-18T09:52:33.427127Z","iopub.status.idle":"2022-07-18T09:52:33.444418Z","shell.execute_reply.started":"2022-07-18T09:52:33.427056Z","shell.execute_reply":"2022-07-18T09:52:33.443420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize = (10,4))\n\nsns.distplot(residuals, ax = ax[0])\nsns.scatterplot(y_true, y_pred, ax = ax[1])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.450196Z","iopub.execute_input":"2022-07-18T09:52:33.453018Z","iopub.status.idle":"2022-07-18T09:52:33.948329Z","shell.execute_reply.started":"2022-07-18T09:52:33.452951Z","shell.execute_reply":"2022-07-18T09:52:33.947377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since plotting the residuals shows a skewed normal distribution, there may be some information that we cannot capture using Regression model","metadata":{}},{"cell_type":"code","source":"#Store prediction\ny_pred_ = model.predict(dfs_td[model_features])\ndfs_td['Age_reg'] = np.round(y_pred_)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.949615Z","iopub.execute_input":"2022-07-18T09:52:33.950110Z","iopub.status.idle":"2022-07-18T09:52:33.965141Z","shell.execute_reply.started":"2022-07-18T09:52:33.950064Z","shell.execute_reply":"2022-07-18T09:52:33.962999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way we can impute Age is by grouping our data by highly correlated variable like Title and Pclass, then find the mean Age within a group to impute","metadata":{}},{"cell_type":"code","source":"def count_na(x):\n    return len(x) - x.count()\n\ntitle_agg = dfs.groupby(['Title', 'Pclass'])['Age'].agg(['mean','count', 'size', count_na]).reset_index()\ntitle_agg","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:33.967800Z","iopub.execute_input":"2022-07-18T09:52:33.968789Z","iopub.status.idle":"2022-07-18T09:52:34.014774Z","shell.execute_reply.started":"2022-07-18T09:52:33.968735Z","shell.execute_reply":"2022-07-18T09:52:34.013920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_p1 = title_agg[title_agg['Pclass'] == 1]\ntitle_p2 = title_agg[title_agg['Pclass'] == 2]\ntitle_p3 = title_agg[title_agg['Pclass'] == 3]\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x= title_p1['Title'], y= title_p1['mean'], customdata = title_p1[['count', 'count_na']], hovertemplate = \"mean = %{y:.0f}<br>count,NaN = %{customdata} \", name = 'Pclass 1'))\nfig.add_trace(go.Bar(x= title_p2['Title'], y= title_p2['mean'], customdata = title_p2[['count', 'count_na']], hovertemplate = 'mean = %{y:.0f}<br>count,NaN = %{customdata}', name = 'Pclass 2'))\nfig.add_trace(go.Bar(x= title_p3['Title'], y= title_p3['mean'], customdata = title_p3[['count', 'count_na']], hovertemplate = 'mean = %{y:.0f}<br>count,NaN = %{customdata}', name = 'Pclass 3'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.016269Z","iopub.execute_input":"2022-07-18T09:52:34.016765Z","iopub.status.idle":"2022-07-18T09:52:34.059928Z","shell.execute_reply.started":"2022-07-18T09:52:34.016717Z","shell.execute_reply":"2022-07-18T09:52:34.058838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we impute with mean within each group, we have to make sure that there are sufficient data within each group to eastimate mean. Only Dr. from Pclass 1 and Ms. from Pclass 3 has no sufficient data to estimate mean, but  there is only 1 NaN in each so it should be fine","metadata":{}},{"cell_type":"code","source":"#All Age with NaN by titles and Pclass\ntitle_agg[title_agg['count_na'] > 0]","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.065846Z","iopub.execute_input":"2022-07-18T09:52:34.066713Z","iopub.status.idle":"2022-07-18T09:52:34.088400Z","shell.execute_reply.started":"2022-07-18T09:52:34.066642Z","shell.execute_reply":"2022-07-18T09:52:34.087763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs[dfs.Title == 'Dr'][['Age','Sex']]","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.089545Z","iopub.execute_input":"2022-07-18T09:52:34.090038Z","iopub.status.idle":"2022-07-18T09:52:34.100870Z","shell.execute_reply.started":"2022-07-18T09:52:34.090004Z","shell.execute_reply":"2022-07-18T09:52:34.100253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs['Age_ME'] = dfs.groupby(['Title', 'Pclass'])['Age'].transform('mean')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.101751Z","iopub.execute_input":"2022-07-18T09:52:34.102640Z","iopub.status.idle":"2022-07-18T09:52:34.115220Z","shell.execute_reply.started":"2022-07-18T09:52:34.102601Z","shell.execute_reply":"2022-07-18T09:52:34.114244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.mean_absolute_error(df_nona['Age'], dfs[dfs.Age.notna()]['Age_ME']))\nprint(metrics.mean_squared_error(df_nona['Age'], dfs[dfs.Age.notna()]['Age_ME']))\nprint(np.sqrt(metrics.mean_squared_error(df_nona['Age'], dfs[dfs.Age.notna()]['Age_ME'])))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.116296Z","iopub.execute_input":"2022-07-18T09:52:34.117115Z","iopub.status.idle":"2022-07-18T09:52:34.128420Z","shell.execute_reply.started":"2022-07-18T09:52:34.117081Z","shell.execute_reply":"2022-07-18T09:52:34.127827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like mean estimation by grouping performs better than regression. We will then manually fill the only missing Pclass 3 Ms with 26. There is a Ms in Pclass 2 with age 28. Since Pclass 2 is known to be generally older than Pclass 3, we wil just guess it is around 26.\n\nPrediction by both methods seems to produce roughly the same result","metadata":{}},{"cell_type":"code","source":"dfs.at[979, 'Age_ME'] = 26\ndfs['Age_reg'] = dfs_td['Age_reg']","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.129424Z","iopub.execute_input":"2022-07-18T09:52:34.130308Z","iopub.status.idle":"2022-07-18T09:52:34.135057Z","shell.execute_reply.started":"2022-07-18T09:52:34.130275Z","shell.execute_reply":"2022-07-18T09:52:34.134230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fillna in Age column with Age_ME\ndfs.Age.fillna(dfs.Age_ME, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.136278Z","iopub.execute_input":"2022-07-18T09:52:34.136604Z","iopub.status.idle":"2022-07-18T09:52:34.146057Z","shell.execute_reply.started":"2022-07-18T09:52:34.136571Z","shell.execute_reply":"2022-07-18T09:52:34.145083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(dfs.Age_ME, dfs.Age_reg, hue = dfs.Pclass)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.150945Z","iopub.execute_input":"2022-07-18T09:52:34.151184Z","iopub.status.idle":"2022-07-18T09:52:34.549018Z","shell.execute_reply.started":"2022-07-18T09:52:34.151150Z","shell.execute_reply":"2022-07-18T09:52:34.548056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(dfs, 'Age')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.550214Z","iopub.execute_input":"2022-07-18T09:52:34.550437Z","iopub.status.idle":"2022-07-18T09:52:34.646461Z","shell.execute_reply.started":"2022-07-18T09:52:34.550408Z","shell.execute_reply":"2022-07-18T09:52:34.645692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fare\nOnly 1 mising value in Fare. Since Fare is mostly correlated with Pclass and if the person is embarked from C","metadata":{}},{"cell_type":"code","source":"dfs[dfs['Ticket'] == '3701']","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.647799Z","iopub.execute_input":"2022-07-18T09:52:34.648178Z","iopub.status.idle":"2022-07-18T09:52:34.668631Z","shell.execute_reply.started":"2022-07-18T09:52:34.648136Z","shell.execute_reply":"2022-07-18T09:52:34.667747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(x = dfs[dfs['Embarked_C'] == 1]['Fare'],name = 'C'))\nfig.add_trace(go.Histogram(x = dfs[dfs['Embarked_Q'] == 1]['Fare'],name = 'Q'))\nfig.add_trace(go.Histogram(x = dfs[dfs['Embarked_S'] == 1]['Fare'],name = 'S'))\n\nfig.update_layout(barmode = 'overlay')\nfig.update_traces(opacity = 0.4)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.670150Z","iopub.execute_input":"2022-07-18T09:52:34.670610Z","iopub.status.idle":"2022-07-18T09:52:34.690957Z","shell.execute_reply.started":"2022-07-18T09:52:34.670565Z","shell.execute_reply":"2022-07-18T09:52:34.689989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since each embarkment place has similar Fare distribution, and this person is embarked from S, we will just fill missing value according to the mean of respective Pclass\n\nIn addition, currently Fare is a total price of a particular ticket# purchased by the whole group. To get a fare each person in the group paid, we need to divide it by the number of people purchasing that particular ticket to get fare per person 'Farepp'.","metadata":{}},{"cell_type":"code","source":"fare_fillna = dfs[(dfs['Pclass'] == 3) & (dfs['Sex'] == 1) & (dfs[['GroupSize', 'famsize']].max(axis=1) == 1)]['Fare'].mean()\ndfs.Fare.fillna(value = fare_fillna, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.692739Z","iopub.execute_input":"2022-07-18T09:52:34.692992Z","iopub.status.idle":"2022-07-18T09:52:34.701878Z","shell.execute_reply.started":"2022-07-18T09:52:34.692960Z","shell.execute_reply":"2022-07-18T09:52:34.700953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs['Farepp'] = dfs['Fare']/dfs['GroupSize']","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.702974Z","iopub.execute_input":"2022-07-18T09:52:34.703220Z","iopub.status.idle":"2022-07-18T09:52:34.713194Z","shell.execute_reply.started":"2022-07-18T09:52:34.703192Z","shell.execute_reply":"2022-07-18T09:52:34.712413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cabin\nNext we will try to fill the missing Cabin data. We will first extract all cabins into groups of Deck by their front letter. Then we are going to try and analyze whether each Deck affects survival rate","metadata":{}},{"cell_type":"code","source":"dfs['Deck'] = dfs['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.714545Z","iopub.execute_input":"2022-07-18T09:52:34.714878Z","iopub.status.idle":"2022-07-18T09:52:34.729030Z","shell.execute_reply.started":"2022-07-18T09:52:34.714834Z","shell.execute_reply":"2022-07-18T09:52:34.728440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deck_agg = dfs.groupby(['Deck', 'Pclass'])[['Survived', 'Name']].agg(['count', 'sum']).reset_index()\ndeck_agg.drop(('Name', 'sum'), axis = 1, inplace = True)\ndeck_agg['relative'] = deck_agg.apply(\n    lambda row: row[('Name', 'count')] / \n    deck_agg[deck_agg['Deck'] == row['Deck'].to_string().strip()][('Name', 'count')].sum(),\n    axis = 1\n)\n\ndef deck_index(Pclass, index):\n    return deck_agg[deck_agg['Pclass'] == Pclass][index]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=deck_index(1, 'Deck'), y= deck_index(1, 'relative'), name='Pclass 1'))\nfig.add_trace(go.Bar(x=deck_index(2, 'Deck'), y= deck_index(2, 'relative'), name='Pclass 2'))\nfig.add_trace(go.Bar(x=deck_index(3, 'Deck'), y= deck_index(3, 'relative'), name='Pclass 3'))\n\nfig.update_layout(barmode = 'relative', \n                  xaxis = {'categoryorder':'array',\n                          'categoryarray':['A','B','C','D','E','F','G','T','M']})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.730306Z","iopub.execute_input":"2022-07-18T09:52:34.730588Z","iopub.status.idle":"2022-07-18T09:52:34.824308Z","shell.execute_reply.started":"2022-07-18T09:52:34.730557Z","shell.execute_reply":"2022-07-18T09:52:34.823507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It would seem that each deck is highly correlated with Pclass so it will definitely have an impact on survival rate but is this information redundant since we already have Pclass variable ?","metadata":{}},{"cell_type":"code","source":"deck_agg['Survival_rate'] = deck_agg[('Survived', 'sum')] / deck_agg[('Survived', 'count')]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=deck_index(1, 'Deck'), y= deck_index(1, 'Survival_rate'),\n                     customdata = deck_index(1, ('Name', 'count')), name='Pclass 1',\n                    hovertemplate = \"Survival rate: %{y:.2f}<br>Count: %{customdata}\"))\nfig.add_trace(go.Bar(x=deck_index(2, 'Deck'), y= deck_index(2, 'Survival_rate'),\n                     customdata = deck_index(2, ('Name', 'count')), name='Pclass 2',\n                    hovertemplate = \"Survival rate: %{y:.2f}<br>Count: %{customdata}\"))\nfig.add_trace(go.Bar(x=deck_index(3, 'Deck'), y= deck_index(3, 'Survival_rate'),\n                     customdata = deck_index(3, ('Name', 'count')), name='Pclass 3',\n                    hovertemplate = \"Survival rate: %{y:.2f}<br>Count: %{customdata}\"))\n\nfig.update_layout(xaxis = {'categoryorder':'array',\n                           'categoryarray':['A','B','C','D','E','F','G','T','M']})\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.825923Z","iopub.execute_input":"2022-07-18T09:52:34.826778Z","iopub.status.idle":"2022-07-18T09:52:34.869014Z","shell.execute_reply.started":"2022-07-18T09:52:34.826732Z","shell.execute_reply":"2022-07-18T09:52:34.868198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a great insight. It would seem that people in deck E has very high survival rate, but not so much in Deck A. This could mean that their position within the ship can determine their survival rate. Cabin does really add extra information on survival rate, although there is so much missing data on this variiable so the data can be very noisy if there is no reliable way to predict the missing value\n\nTo make this easier we will first group each Deck together according to the proximity and sililarity in survival rate as well as Pclass composition. There will be ABC, DE, FG group and we will lump individual in cabin T to ABC group as he belongs to Pclass 1.\n\nThose with missing value will be grouped together as M, and we will also try to subgroup the M missing deck according to their P class since there is an inherent difference in their possible deck and survival rate from Pclass.","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:36:09.93124Z","iopub.execute_input":"2022-07-08T09:36:09.931776Z","iopub.status.idle":"2022-07-08T09:36:09.935322Z","shell.execute_reply.started":"2022-07-08T09:36:09.931739Z","shell.execute_reply":"2022-07-08T09:36:09.934509Z"}}},{"cell_type":"code","source":"deckdict = {'A':'ABC', 'B':'ABC', 'C':'ABC', 'D':'DE', 'E':'DE', 'F':'FG',\n           'G':'FG', 'T':'ABC', 'M': np.NaN}\ndfs['DeckGroup'] = dfs.Deck.map(deckdict)\n\ndfs.loc[(dfs.DeckGroup == 'M') & (dfs.Pclass == 1), 'DeckGroup'] = 'M1'\ndfs.loc[(dfs.DeckGroup == 'M') & (dfs.Pclass == 2), 'DeckGroup'] = 'M2'\ndfs.loc[(dfs.DeckGroup == 'M') & (dfs.Pclass == 3), 'DeckGroup'] = 'M3'","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.870541Z","iopub.execute_input":"2022-07-18T09:52:34.871070Z","iopub.status.idle":"2022-07-18T09:52:34.883185Z","shell.execute_reply.started":"2022-07-18T09:52:34.871026Z","shell.execute_reply":"2022-07-18T09:52:34.882539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To fill in missing data, we will start to see the dataset and see whether Cabin can be inferred from Ticket or Fare or by Family name","metadata":{}},{"cell_type":"code","source":"def displayall(data, columns, sort):\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n        display(data[columns].sort_values(sort))\n\ndef dataselect(data, columns, sort):\n    return data[columns].sort_values(sort)\n\ndef displayhtml(data):\n    display(HTML(\"<div style='height: 200px; overflow: auto; width: fit-content'>\" +\n                 data.to_html() + \"</div>\"))\n    \ndfs_views = dataselect(dfs, ['Name', 'Pclass', 'DeckGroup', 'Ticket', 'Farepp', 'Embarked', 'GroupSize'],\n          ['Pclass','Farepp', 'Ticket'])\n\ndisplayhtml(dfs_views)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:34.884305Z","iopub.execute_input":"2022-07-18T09:52:34.885128Z","iopub.status.idle":"2022-07-18T09:52:35.026922Z","shell.execute_reply.started":"2022-07-18T09:52:34.885084Z","shell.execute_reply":"2022-07-18T09:52:35.026182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like some tickets with same number can have different cabin, and Last name seems to not be unique to a family. In addition sometimes people with same Fare price can have different deck, so it will not be possible to accurately infer Cabin information from other features, but we can still infer to it to some extent. However, since there is a huge chunk of missing data it is better to leave this variable out because it would have a lot of noise\n\nWe can still try to fill it by their relative Pclass and Farepp using front propagation method. Alternatively, we can just create a new variable that indicatse whether we have information available for the Cabin","metadata":{"execution":{"iopub.status.busy":"2022-07-08T09:36:11.628085Z","iopub.execute_input":"2022-07-08T09:36:11.628421Z","iopub.status.idle":"2022-07-08T09:36:11.634103Z","shell.execute_reply.started":"2022-07-08T09:36:11.62839Z","shell.execute_reply":"2022-07-08T09:36:11.632334Z"}}},{"cell_type":"code","source":"dfs['DeckGroup'] = dfs.sort_values(['Pclass','Farepp', 'Ticket'])['DeckGroup'].ffill()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.027861Z","iopub.execute_input":"2022-07-18T09:52:35.028576Z","iopub.status.idle":"2022-07-18T09:52:35.039831Z","shell.execute_reply.started":"2022-07-18T09:52:35.028526Z","shell.execute_reply":"2022-07-18T09:52:35.038719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n## Grouping variables: Title\nSince there are many titles that has few people, it can introduce noise to the model. We can group some titles togehter to make it less noisy","metadata":{}},{"cell_type":"code","source":"px.histogram(dfs[:890], x='Title', color='Survived')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.042109Z","iopub.execute_input":"2022-07-18T09:52:35.042637Z","iopub.status.idle":"2022-07-18T09:52:35.117915Z","shell.execute_reply.started":"2022-07-18T09:52:35.042561Z","shell.execute_reply":"2022-07-18T09:52:35.117147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Whole Set:\\n',dfs.Title.value_counts())\nprint('Train Set:\\n',dfs[:890].Title.value_counts())\nprint('Test Set:\\n',dfs[891:].Title.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.119087Z","iopub.execute_input":"2022-07-18T09:52:35.119322Z","iopub.status.idle":"2022-07-18T09:52:35.130390Z","shell.execute_reply.started":"2022-07-18T09:52:35.119294Z","shell.execute_reply":"2022-07-18T09:52:35.129595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TitleDict = {'Mr':'Mr', 'Miss':'Miss', 'Mrs':'Mrs', 'Master':'Children',\n            'FemaleChild':'Children', 'Dr':'Smart', 'Rev':'Mr',\n             'Capt':'Smart', 'Johnkheer':'Royalty', 'Don':'Royalty',\n             'Mlle':'Miss', 'Major':'Smart', 'Col':'Smart', 'Countess':'Royalty',\n             'Lady':'Royalty', 'Mme':'Miss', 'Dona':'Royalty','Sir':'Royalty'\n            }\n\ndfs['Title_g'] = dfs['Title'].map(TitleDict)\ndfs['Title_g'] = dfs['Title'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.131787Z","iopub.execute_input":"2022-07-18T09:52:35.132414Z","iopub.status.idle":"2022-07-18T09:52:35.145767Z","shell.execute_reply.started":"2022-07-18T09:52:35.132379Z","shell.execute_reply":"2022-07-18T09:52:35.144788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Family Size information\nWe will add 'is_alone' column to indicate that the person is alone when she has no Parch or SibSp. Somebody who is alone might have a better chance at survival. We also add 'famsize' variable that adds together Parch, SibSp and him/herself. max_size will be added for which we will get the max value between Family size and Group Size (From Parch/SibSp and from Ticket#)","metadata":{}},{"cell_type":"code","source":"def add_isalone_column(dataset):\n    dataset['is_alone'] = np.where(dataset['Parch'] + dataset['SibSp'] == 0, 1, 0)\n\nadd_isalone_column(dfs)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.147246Z","iopub.execute_input":"2022-07-18T09:52:35.147651Z","iopub.status.idle":"2022-07-18T09:52:35.155481Z","shell.execute_reply.started":"2022-07-18T09:52:35.147609Z","shell.execute_reply":"2022-07-18T09:52:35.154731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs['max_size'] = dfs[['GroupSize', 'famsize']].max(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.156733Z","iopub.execute_input":"2022-07-18T09:52:35.157134Z","iopub.status.idle":"2022-07-18T09:52:35.167109Z","shell.execute_reply.started":"2022-07-18T09:52:35.157092Z","shell.execute_reply":"2022-07-18T09:52:35.166120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows = 1, cols = 2,\n                   subplot_titles=('Survival by max_size',\n                                  'max_size distribution'))\n\nfig1 = px.histogram(dfs[0:891], x='max_size', color='Survived', barmode='group')\nfig2 = px.histogram(dfs, x='max_size', color_discrete_sequence=['navy'])\n\ndef plot_px(figure, row, col):\n    fig_trace = []\n    for trace in range(len(figure[\"data\"])):\n        fig_trace.append(figure[\"data\"][trace])\n    for traces in fig_trace:\n        fig.append_trace(traces, row=row, col=col)\n\nplot_px(fig1, 1, 1)\nplot_px(fig2, 1, 2)\n\nfig.update_xaxes(dtick = 1)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.168538Z","iopub.execute_input":"2022-07-18T09:52:35.168794Z","iopub.status.idle":"2022-07-18T09:52:35.305680Z","shell.execute_reply.started":"2022-07-18T09:52:35.168762Z","shell.execute_reply":"2022-07-18T09:52:35.304837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grouping variables: max_size\nWe are going to label the group size by solo, duo, small, medium and large to reduce noise. Since we can capture whether the person is alone in this variable, is_alone variable won't be nneeded anymore","metadata":{}},{"cell_type":"code","source":"sizedict = {1:'solo', 2:'duo', 3:'small', 4:'small', 5:'medium', 6:'medium',\n          7:'medium', 8:'medium', 11:'large'}\n\ndfs['SizeLabel'] = dfs.max_size.map(sizedict)\ndfs['SizeLabel'] = dfs['SizeLabel'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.306930Z","iopub.execute_input":"2022-07-18T09:52:35.309681Z","iopub.status.idle":"2022-07-18T09:52:35.318619Z","shell.execute_reply.started":"2022-07-18T09:52:35.309632Z","shell.execute_reply":"2022-07-18T09:52:35.317866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binning variables: Fare\nBinning variables can help reduce noise and may add more information to the model. We will observe the distribution of Farepp and bin them according to the distribution wall of data","metadata":{}},{"cell_type":"code","source":"#Distribution of Farepp\nfig = make_subplots(rows=1, cols=2)\nbox = px.box(dfs.Farepp)\n\nfig.add_trace(go.Histogram(x = dfs.Farepp), row = 1, col = 1)\nplot_px(box, 1, 2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.319744Z","iopub.execute_input":"2022-07-18T09:52:35.320134Z","iopub.status.idle":"2022-07-18T09:52:35.439299Z","shell.execute_reply.started":"2022-07-18T09:52:35.320097Z","shell.execute_reply":"2022-07-18T09:52:35.438422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Discretize fare value into sections according to the distribution wall of data\n#Observing the walls at\n#right = false\n#0, 9, 13, 17, 31, 53, max of 128.0823\n\ndef binning(dataset, varname, binvar, ranges, right=False, labels=False):\n    dataset[varname], bins = pd.cut(dataset[binvar], ranges, labels=labels, retbins=True,\n                                   include_lowest = True, right=right)\n    return bins\n\ndef add_Fareq_column(dataset, quartile):\n    dataset['Fareq'], bins = pd.qcut(dataset.Farepp, quartile, labels=False, retbins=True)\n    return bins\n\nbins = binning(dfs, 'Fareq', 'Farepp', [0, 9, 13, 17, 31, 53, 129])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.440415Z","iopub.execute_input":"2022-07-18T09:52:35.440692Z","iopub.status.idle":"2022-07-18T09:52:35.453086Z","shell.execute_reply.started":"2022-07-18T09:52:35.440660Z","shell.execute_reply":"2022-07-18T09:52:35.452172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Binning variables: Age\nWe can also bin age the same way we bin Fare","metadata":{}},{"cell_type":"code","source":"#Distribution of Age\nfig = make_subplots(rows=1, cols=2)\nbox = px.box(dfs.Age)\n\nfig.add_trace(go.Histogram(x = dfs.Age), row = 1, col = 1)\nplot_px(box, 1, 2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.454098Z","iopub.execute_input":"2022-07-18T09:52:35.454302Z","iopub.status.idle":"2022-07-18T09:52:35.546716Z","shell.execute_reply.started":"2022-07-18T09:52:35.454277Z","shell.execute_reply":"2022-07-18T09:52:35.546015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs['Age_bin'], bins = pd.qcut(dfs.Age, 10, labels=False, retbins = True)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.547862Z","iopub.execute_input":"2022-07-18T09:52:35.548085Z","iopub.status.idle":"2022-07-18T09:52:35.554188Z","shell.execute_reply.started":"2022-07-18T09:52:35.548056Z","shell.execute_reply":"2022-07-18T09:52:35.553592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Information from members of the Group / Family\nWe need to consider the survival of people in the same group which is same family or same ticket no. If someone in the group is known to survive then the rest of the group will also be more likely to survive. We will first extract family name from the Name column, then we will try to extract more information about survivor in the same Ticket# / family.\n\nWe can extract\n- FSR: Family survival rate\n- TSR: Group survival rate\n- FNA / TNA: Boolean whether FSR/TSR is not available\n- SR: Average between FSR and TSR\n- SR_NA: boolean whether there is more than 1 people in the group/family\n\n\n","metadata":{}},{"cell_type":"code","source":"dfs['Family'] = dfs.Name.str.extract(r'(.*),')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.555365Z","iopub.execute_input":"2022-07-18T09:52:35.555609Z","iopub.status.idle":"2022-07-18T09:52:35.571504Z","shell.execute_reply.started":"2022-07-18T09:52:35.555580Z","shell.execute_reply":"2022-07-18T09:52:35.570428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make survival rate for family group\nfamily_group = dfs.groupby('Family')[['Survived','Name']].agg(['count', 'sum']).reset_index()\nfamily_group.columns = [' '.join(col).strip() for col in family_group.columns.values]\nfamily_group.columns = ['Family', 'Survived count', 'Survived sum', 'Family Size', 'drop']\nfamily_group.drop(columns = 'drop', inplace=True)\nfamily_group['FSR'] = family_group['Survived sum'] / family_group['Survived count']\nfamily_group['NA_count'] = family_group['Family Size'] - family_group['Survived count']\nfamily_group['FNA'] = (family_group['Survived count'] == 0)\nfamily_group.loc[family_group['Family Size'] == 1, 'FNA'] = True","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.572671Z","iopub.execute_input":"2022-07-18T09:52:35.573262Z","iopub.status.idle":"2022-07-18T09:52:35.596909Z","shell.execute_reply.started":"2022-07-18T09:52:35.573215Z","shell.execute_reply":"2022-07-18T09:52:35.596033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make survival rate for people with same Ticket number\nticket_group = dfs.groupby('Ticket')[['Survived','Family']].agg(['count', 'sum']).reset_index()\nticket_group.columns = ['Ticket', 'Survived count', 'Survived sum', 'Ticket size', 'Family']\nticket_group['TSR'] = ticket_group['Survived sum'] / ticket_group['Survived count']\nticket_group['NA_count'] = ticket_group['Ticket size'] - ticket_group['Survived count']\nticket_group['TNA'] = (ticket_group['Survived count'] == 0)\nticket_group.loc[ticket_group['Ticket size'] == 1, 'TNA'] = True","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.598113Z","iopub.execute_input":"2022-07-18T09:52:35.598365Z","iopub.status.idle":"2022-07-18T09:52:35.618423Z","shell.execute_reply.started":"2022-07-18T09:52:35.598335Z","shell.execute_reply":"2022-07-18T09:52:35.617810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make a dict of survival rate\nFSR_dict = dict(zip(family_group['Family'], family_group['FSR']))\nFNA_dict = dict(zip(family_group['Family'], family_group['FNA']))\nTSR_dict = dict(zip(ticket_group['Ticket'], ticket_group['TSR']))\nTNA_dict = dict(zip(ticket_group['Ticket'], ticket_group['TNA']))\n\n#Map survival rate to the main dataset\ndfs['FSR'] = dfs.Family.map(FSR_dict)\ndfs['FNA'] = dfs.Family.map(FNA_dict)\ndfs['TSR'] = dfs.Ticket.map(TSR_dict)\ndfs['TNA'] = dfs.Ticket.map(TNA_dict)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.620878Z","iopub.execute_input":"2022-07-18T09:52:35.621304Z","iopub.status.idle":"2022-07-18T09:52:35.637467Z","shell.execute_reply.started":"2022-07-18T09:52:35.621263Z","shell.execute_reply":"2022-07-18T09:52:35.636843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From inspecting the dataset, there are cases where either FNA / TNA is True, so we can just use the information of what is available if there are both data available, we will take average into SR variable","metadata":{}},{"cell_type":"code","source":"displayhtml(dataselect(dfs, ['FSR','FNA','TSR','TNA'], ['FNA', 'TNA']))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.638648Z","iopub.execute_input":"2022-07-18T09:52:35.639485Z","iopub.status.idle":"2022-07-18T09:52:35.732373Z","shell.execute_reply.started":"2022-07-18T09:52:35.639422Z","shell.execute_reply":"2022-07-18T09:52:35.731365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate SR\n\nfemalemean = dfs[dfs.Sex == 0].Survived.mean()\nmalemean = dfs[dfs.Sex == 1].Survived.mean()\n\ndef calculate_SR(df):\n    if df.TNA == True and df.FNA == False:\n        return df.FSR\n    elif df.TNA == False and df.FNA == True:\n        return df.TSR\n    elif df.TNA == False and df.FNA == False:\n        return np.mean((df.TSR, df.FSR))\n    else:\n        if df.Sex == 0:\n            return femalemean\n        else:\n            return malemean\n\ndfs['SR'] = dfs.apply(calculate_SR, axis = 1)\n\n#Column to indicate whether our SR is NA\ndfs['SR_NA'] = (dfs.TNA == True) & (dfs.FNA == True)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.733582Z","iopub.execute_input":"2022-07-18T09:52:35.733816Z","iopub.status.idle":"2022-07-18T09:52:35.810194Z","shell.execute_reply.started":"2022-07-18T09:52:35.733789Z","shell.execute_reply":"2022-07-18T09:52:35.809513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayhtml(dataselect(dfs, ['FSR','FNA','TSR','TNA','SR', 'SR_NA'], ['FNA', 'TNA']))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.811128Z","iopub.execute_input":"2022-07-18T09:52:35.812104Z","iopub.status.idle":"2022-07-18T09:52:35.936401Z","shell.execute_reply.started":"2022-07-18T09:52:35.812041Z","shell.execute_reply":"2022-07-18T09:52:35.935517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these we can make another variable 'FamilySurvived' to indicate\n- -1 if no SR is available\n- 0 if SR is 0\n- 1 if SR is > 0 or someone survives in the same group/family","metadata":{}},{"cell_type":"code","source":"dfs['FamilySurvived'] = np.where(dfs['SR'] > 0, 1, 0)\ndfs['FamilySurvived'] = np.where(dfs['SR_NA'] == True, -1, dfs['FamilySurvived'])","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.937543Z","iopub.execute_input":"2022-07-18T09:52:35.937772Z","iopub.status.idle":"2022-07-18T09:52:35.943989Z","shell.execute_reply.started":"2022-07-18T09:52:35.937744Z","shell.execute_reply":"2022-07-18T09:52:35.943478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayhtml(dataselect(dfs, ['SR', 'SR_NA', 'FamilySurvived'], ['SR']))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:35.944838Z","iopub.execute_input":"2022-07-18T09:52:35.945559Z","iopub.status.idle":"2022-07-18T09:52:36.021573Z","shell.execute_reply.started":"2022-07-18T09:52:35.945525Z","shell.execute_reply":"2022-07-18T09:52:36.020684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try to bin SR into categorical variable:\n\n- 0 = No info\n- 1 = 0 SR all died\n- 2 = 0.1 - 0.5 SR\n- 3 = 0.51 - 0.7 SR\n- 4 = 0.71 - 0.99 SR\n- 5 = 1 SR all survived","metadata":{}},{"cell_type":"code","source":"sns.distplot(dfs[dfs.SR_NA == False].SR, bins = 20)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.023036Z","iopub.execute_input":"2022-07-18T09:52:36.023398Z","iopub.status.idle":"2022-07-18T09:52:36.299495Z","shell.execute_reply.started":"2022-07-18T09:52:36.023356Z","shell.execute_reply":"2022-07-18T09:52:36.298649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scalar_list = [0, 0.1, 0.5, 0.7, 0.99, 1]\nlabel = list(range(1,len(scalar_list)))\n\nbins = binning(dfs, 'SR_cat', 'SR', scalar_list, labels = [str(i) for i in label],\n               right=True)\ndfs['SR_cat'] = dfs['SR_cat'].astype('category')\ndfs['SR_cat'] = dfs['SR_cat'].cat.add_categories('0')\ndfs['SR_cat'] = dfs['SR_cat'].cat.reorder_categories([str(i) for i in range(len(scalar_list))],\n                                                     ordered = True)\ndfs.loc[dfs.SR_NA == True, 'SR_cat'] = '0'","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.300806Z","iopub.execute_input":"2022-07-18T09:52:36.301603Z","iopub.status.idle":"2022-07-18T09:52:36.316288Z","shell.execute_reply.started":"2022-07-18T09:52:36.301550Z","shell.execute_reply":"2022-07-18T09:52:36.315397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayhtml(dataselect(dfs, ['SR', 'SR_cat', 'SR_NA'], ['SR_cat', 'SR']))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.317713Z","iopub.execute_input":"2022-07-18T09:52:36.318202Z","iopub.status.idle":"2022-07-18T09:52:36.401266Z","shell.execute_reply.started":"2022-07-18T09:52:36.318159Z","shell.execute_reply":"2022-07-18T09:52:36.400693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayhtml(dataselect(dfs, ['SR', 'SR_cat'], ['SR_cat', 'SR']))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.402343Z","iopub.execute_input":"2022-07-18T09:52:36.402584Z","iopub.status.idle":"2022-07-18T09:52:36.462664Z","shell.execute_reply.started":"2022-07-18T09:52:36.402554Z","shell.execute_reply":"2022-07-18T09:52:36.461745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = dfs[0:891]\npx.histogram(train_data, x='SR_cat', color='Survived', barmode = 'group',\n            category_orders=dict(SR_cat=[str(i) for i in range(len(scalar_list))]))","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.463894Z","iopub.execute_input":"2022-07-18T09:52:36.464128Z","iopub.status.idle":"2022-07-18T09:52:36.531967Z","shell.execute_reply.started":"2022-07-18T09:52:36.464099Z","shell.execute_reply":"2022-07-18T09:52:36.531059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n## Creating dummy variables\nWe will choose all engineered and/or highly correlated features into the model. We need to convert categorical or object variable into dummy variables first before we feed it into a model\n","metadata":{}},{"cell_type":"code","source":"#Choosing features in a model\nfeatures = [\"Pclass\",\"Sex\",\"SizeLabel\",\"Age_bin\",\"Title_g\",'Fareq','FamilySurvived']\ndfs_data = dfs[features]\ndfs_dummy = pd.get_dummies(dfs_data)\n\n#RF\nX = dfs_dummy[0:891]\nX_test = dfs_dummy[891:]\n\n#NN\ntrain_data = dfs[0:891]\ntest_data = dfs[891:]\n\n#target\ny = train_data[\"Survived\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.533672Z","iopub.execute_input":"2022-07-18T09:52:36.534289Z","iopub.status.idle":"2022-07-18T09:52:36.547575Z","shell.execute_reply.started":"2022-07-18T09:52:36.534246Z","shell.execute_reply":"2022-07-18T09:52:36.546662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking NaN value in Train and Test dataset\nfor name, data in {'Training Data':train_data, 'Test Data':test_data}.items():\n    print(f'For {name}:')\n    for col in data.columns:\n        print(f\"{col} has {data[col].isna().sum()} NaN\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.548963Z","iopub.execute_input":"2022-07-18T09:52:36.549230Z","iopub.status.idle":"2022-07-18T09:52:36.587097Z","shell.execute_reply.started":"2022-07-18T09:52:36.549199Z","shell.execute_reply":"2022-07-18T09:52:36.586442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RandomForest Model\nWe will use simple random forest model as a baseline for our prediction","metadata":{}},{"cell_type":"code","source":"#Baseline model as initially given\nRF_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nRF_baseline = RF_model.fit(X, y)\npredictions = RF_baseline.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('RF_baseline.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.588012Z","iopub.execute_input":"2022-07-18T09:52:36.588619Z","iopub.status.idle":"2022-07-18T09:52:36.771195Z","shell.execute_reply.started":"2022-07-18T09:52:36.588578Z","shell.execute_reply":"2022-07-18T09:52:36.770160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will tune this model using GridSearchCV and we will tune from the most important features first to the least:\n- max_depth and min_samples_split\n- min_samples_leaf and max_samples\n\nPrediction scores should improved in each tuning.","metadata":{}},{"cell_type":"code","source":"#Tuning Hyperparameter of Random Forest Classifier\npprint(RF_model.get_params())\n\n#Tuning function\ndef tune(model, param):\n    gsearch = GridSearchCV(estimator = model, param_grid = param,\n                             scoring = 'roc_auc', n_jobs = -1, cv = 4)\n    gsearch.fit(X, y)\n    print (gsearch.best_params_, gsearch.best_score_)\n    model.set_params(**gsearch.best_params_)\n    return gsearch","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.772545Z","iopub.execute_input":"2022-07-18T09:52:36.773282Z","iopub.status.idle":"2022-07-18T09:52:36.788180Z","shell.execute_reply.started":"2022-07-18T09:52:36.773237Z","shell.execute_reply":"2022-07-18T09:52:36.787238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tune max_depth and min_samples_split\nparam_grid = {'max_depth':range(3,15),\n             'min_samples_split':range(2,10)}\nRF_tuned = RF_model\ngsearch = tune(RF_tuned,param_grid)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:52:36.789382Z","iopub.execute_input":"2022-07-18T09:52:36.789624Z","iopub.status.idle":"2022-07-18T09:53:08.351787Z","shell.execute_reply.started":"2022-07-18T09:52:36.789596Z","shell.execute_reply":"2022-07-18T09:53:08.350754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tune min_samples_leaf and max_samples\nparam_grid = {'min_samples_leaf': range(1,8),\n             'max_samples':[i / 100 for i in range(70,100,5)]}\ngsearch = tune(RF_tuned,param_grid)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:08.355523Z","iopub.execute_input":"2022-07-18T09:53:08.356078Z","iopub.status.idle":"2022-07-18T09:53:20.910911Z","shell.execute_reply.started":"2022-07-18T09:53:08.356030Z","shell.execute_reply":"2022-07-18T09:53:20.910023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#n_estimators\nparam_grid = {'n_estimators':range(100,2001,200)}\ngsearch = tune(RF_tuned,param_grid)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:20.912059Z","iopub.execute_input":"2022-07-18T09:53:20.912275Z","iopub.status.idle":"2022-07-18T09:53:50.050630Z","shell.execute_reply.started":"2022-07-18T09:53:20.912249Z","shell.execute_reply":"2022-07-18T09:53:50.049694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print param of tuned RF\npprint(RF_tuned.get_params())\n\n#Prediction from Tuned RF\npredictions = RF_tuned.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('RF_tuned.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.052895Z","iopub.execute_input":"2022-07-18T09:53:50.053138Z","iopub.status.idle":"2022-07-18T09:53:50.085017Z","shell.execute_reply.started":"2022-07-18T09:53:50.053100Z","shell.execute_reply":"2022-07-18T09:53:50.084018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Classifier\nXGBoost is a more complex tree based classifier\nWe will try to use this more complex model and tune it the same way as we did earlier for RF, from the most impactful parameter to the least. We will first have to find an optimal number of tree for high learning rate so that we can tune these parameters quickly. We will tune regularization parameter last before we decrease learning rate and increase trees.","metadata":{}},{"cell_type":"code","source":"\nxgb_model = xgb.XGBClassifier(learning_rate = 0.01, random_state=1)\nxgb_baseline = xgb_model.fit(X, y)\npredictions = xgb_baseline.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('XGB_baseline.csv', index=False)\nprint(\"Your XGBbaseline model was successfully saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.086385Z","iopub.execute_input":"2022-07-18T09:53:50.088053Z","iopub.status.idle":"2022-07-18T09:53:50.523745Z","shell.execute_reply.started":"2022-07-18T09:53:50.088018Z","shell.execute_reply":"2022-07-18T09:53:50.522685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nxgb1 = XGBClassifier(learning_rate = 0.1,\nmax_depth = 5,\nmin_child_weight = 1,\ngamma = 0,\nsubsample = 0.8,\ncolsample_bytree = 0.8,\nobjective = 'binary:logistic',\nscale_pos_weight = 1,\nuse_label_encoder = False,\nseed = 63)\n\n#Set high LR and using CV to find optimal number of trees\nxgb_param = xgb1.get_xgb_params()\nxgbtrain = xgb.DMatrix(X, label = y, enable_categorical = True)\ncv_result = xgb.cv(xgb_param, xgbtrain, num_boost_round = 1000, metrics = 'auc', \n       early_stopping_rounds = 50)\n\nxgb1.set_params(n_estimators = cv_result.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.524806Z","iopub.execute_input":"2022-07-18T09:53:50.525012Z","iopub.status.idle":"2022-07-18T09:53:50.532439Z","shell.execute_reply.started":"2022-07-18T09:53:50.524986Z","shell.execute_reply":"2022-07-18T09:53:50.531527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Tune max_depth and min_child weight\nparam_grid = {'max_depth':range(9, 12, 1) , 'min_child_weight':range(1,3,1)}\ngsearch = tune(xgb1, param_grid)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.533939Z","iopub.execute_input":"2022-07-18T09:53:50.534256Z","iopub.status.idle":"2022-07-18T09:53:50.548679Z","shell.execute_reply.started":"2022-07-18T09:53:50.534215Z","shell.execute_reply":"2022-07-18T09:53:50.547803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Tune Gamma\nparam_grid = {'gamma':[i / 10 for i in range(0,5)]}\ngsearch = tune(xgb1, param_grid)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.550045Z","iopub.execute_input":"2022-07-18T09:53:50.551198Z","iopub.status.idle":"2022-07-18T09:53:50.559028Z","shell.execute_reply.started":"2022-07-18T09:53:50.551149Z","shell.execute_reply":"2022-07-18T09:53:50.558350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Tune subsample and colsample_bytree\nparam_grid = {'subsample':[i/10 for i in range(6,10)],\n              'colsample_bytree':[i/10 for i in range(6,10)]}\ngsearch = tune(xgb1, param_grid)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.560103Z","iopub.execute_input":"2022-07-18T09:53:50.560807Z","iopub.status.idle":"2022-07-18T09:53:50.571231Z","shell.execute_reply.started":"2022-07-18T09:53:50.560762Z","shell.execute_reply":"2022-07-18T09:53:50.570251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Further tuning subsample and colsample_bytree around previous best params\nparam_grid = {'subsample':[i/100 for i in range(80,96,5)],\n              'colsample_bytree':[i/100 for i in range(80,96,5)]}\ngsearch = tune(xgb1, param_grid)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.572823Z","iopub.execute_input":"2022-07-18T09:53:50.573524Z","iopub.status.idle":"2022-07-18T09:53:50.580080Z","shell.execute_reply.started":"2022-07-18T09:53:50.573480Z","shell.execute_reply":"2022-07-18T09:53:50.579281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Tuning Regularization Parameters\nparam_grid = {'reg_alpha':[0.01, 0.1],\n              'reg_lambda':[1.3]}\ngsearch = tune(xgb1, param_grid)\n#alpha = 0.0001, lambda = 1 best, not needed\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.581118Z","iopub.execute_input":"2022-07-18T09:53:50.581343Z","iopub.status.idle":"2022-07-18T09:53:50.591777Z","shell.execute_reply.started":"2022-07-18T09:53:50.581315Z","shell.execute_reply":"2022-07-18T09:53:50.590774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Decreasing learning rate and increase trees\nxgb_param = xgb1.get_xgb_params()\nxgb_param['learning_rate'] = 0.001\ncv_result = xgb.cv(xgb_param, xgbtrain, num_boost_round = 10000, metrics = 'auc', \n       early_stopping_rounds = 1500)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.593008Z","iopub.execute_input":"2022-07-18T09:53:50.593283Z","iopub.status.idle":"2022-07-18T09:53:50.604349Z","shell.execute_reply.started":"2022-07-18T09:53:50.593247Z","shell.execute_reply":"2022-07-18T09:53:50.603438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#So far, max depth = 9, minchildweight = 1, gamma = 0, colsample and subsample = 0.8\n#reg_lambda = 0.4, reg_alpha = 0.01, LR = 0.001, n_estimators = 3014\nxgb1.set_params(max_depth = 9, min_child_weight = 1, gamma = 0, reg_lambda = 1.3,\n                reg_alpha = 0.01,\n               colsample_bytree = 0.85, subsample = 0.85, learning_rate = 0.001,\n               n_estimators = 2520)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.623476Z","iopub.execute_input":"2022-07-18T09:53:50.624226Z","iopub.status.idle":"2022-07-18T09:53:50.633882Z","shell.execute_reply.started":"2022-07-18T09:53:50.624184Z","shell.execute_reply":"2022-07-18T09:53:50.632796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Set learning rate and boosting number\nxgb1.set_params(learning_rate = 0.001, n_estimators = cv_result.shape[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.635611Z","iopub.execute_input":"2022-07-18T09:53:50.636326Z","iopub.status.idle":"2022-07-18T09:53:50.643957Z","shell.execute_reply.started":"2022-07-18T09:53:50.636279Z","shell.execute_reply":"2022-07-18T09:53:50.643417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Fit the model and predict\nxgb1.fit(X, y)\npredictions = xgb1.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('XGBoost_tuned.csv', index=False)\nprint(\"Your XGBoost model was successfully saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:53:50.645140Z","iopub.execute_input":"2022-07-18T09:53:50.645583Z","iopub.status.idle":"2022-07-18T09:54:00.055424Z","shell.execute_reply.started":"2022-07-18T09:53:50.645548Z","shell.execute_reply":"2022-07-18T09:54:00.054482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network\nWe will try to use simple neural network model to predict.\nWe set a high number of nodes since, having high nodes doesn't impact the model negatively in terms of prediction\n\nWe need to split train/test from the train data, since there are only 891 data points on train set. Data is splitted into 70%-TRAIN 30%VALIDATION set.\nDev set is not neccessary as we can use the real test set as test set.\nWe will also shuffle our dataset before the split.","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:00.064522Z","iopub.execute_input":"2022-07-18T09:54:00.064853Z"}}},{"cell_type":"code","source":"def nn_model():\n    model = keras.Sequential([\n        layers.Dense(100, activation = 'LeakyReLU'),\n        layers.Dense(100, activation = 'LeakyReLU'),\n        layers.Dense(50, activation = 'LeakyReLU'),\n        layers.Dense(1, activation = 'sigmoid')\n    ])\n\n    model.compile(optimizer = 'adam',\n                    loss = 'binary_crossentropy',\n                    metrics = ['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-07-18T10:22:57.200690Z","iopub.execute_input":"2022-07-18T10:22:57.201061Z","iopub.status.idle":"2022-07-18T10:22:57.208320Z","shell.execute_reply.started":"2022-07-18T10:22:57.201020Z","shell.execute_reply":"2022-07-18T10:22:57.207515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_data = train_data.sample(frac=1, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:06.711856Z","iopub.execute_input":"2022-07-18T09:54:06.712082Z","iopub.status.idle":"2022-07-18T09:54:06.734756Z","shell.execute_reply.started":"2022-07-18T09:54:06.712057Z","shell.execute_reply":"2022-07-18T09:54:06.734020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Then we will get our features and label from our data and convert into tensor\ndef extract(dataset, features, test=False):\n    X = pd.get_dummies(dataset[features])\n    if test == False:\n        y = dataset[\"Survived\"]\n        return X, y\n    if test == True:\n        return X\n\ndef make_train_test_set(split):\n    split = int(split * len(s_data))\n    training_set = s_data[:split]\n    val_set = s_data[split:]\n\n    train_X, train_y = extract(training_set, features)\n    test_X, test_y = extract(val_set, features)\n    return train_X, train_y, test_X, test_y\n\ntrain_X, train_y, test_X, test_y = make_train_test_set(0.7)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T10:02:01.452292Z","iopub.execute_input":"2022-07-18T10:02:01.454216Z","iopub.status.idle":"2022-07-18T10:02:01.485968Z","shell.execute_reply.started":"2022-07-18T10:02:01.454149Z","shell.execute_reply":"2022-07-18T10:02:01.485121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(6)\nset_seed(6)\n\n#Now we fit the model\nnn_model1 = nn_model()\nnn_model1.fit(train_X, train_y, epochs = 50, batch_size = 16)\nnn_model1.evaluate(test_X, test_y)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T10:37:23.116115Z","iopub.execute_input":"2022-07-18T10:37:23.116742Z","iopub.status.idle":"2022-07-18T10:37:29.314859Z","shell.execute_reply.started":"2022-07-18T10:37:23.116690Z","shell.execute_reply":"2022-07-18T10:37:29.313971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprediction = nn_model1.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': prediction.flatten()})\noutput['Survived'] = output['Survived'] > 0.5\noutput['Survived'] = output['Survived'].astype(int)\noutput.to_csv('NN_model.csv', index=False)\nprint(\"Your NN_model model was successfully saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-18T10:07:42.583293Z","iopub.execute_input":"2022-07-18T10:07:42.583654Z","iopub.status.idle":"2022-07-18T10:07:42.754863Z","shell.execute_reply.started":"2022-07-18T10:07:42.583617Z","shell.execute_reply":"2022-07-18T10:07:42.753760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM\nAnother tree-based model but less complex and faster than XGBoost","metadata":{}},{"cell_type":"code","source":"gmb_model = gbm.LGBMClassifier(learning_rate = 0.1, random_state=1)\n\nlgbm_model = gmb_model.fit(X, y)\npredictions = lgbm_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions.astype(int)})\noutput.to_csv('LGBM.csv', index=False)\nprint(\"Your LGBM model was successfully saved!\")\n\n'''#Cross Validating using different learning rate for the model\ndef cross_validation(model, X, y, folds):\n    cv_results = cross_validate(model, X, y, cv = folds)\n    return cv_results['test_score']\nX_cross = pd.get_dummies(train_data[features])\ny_cross = train_data[\"Survived\"]\nlearning_rate = [0.09, 0.1, 0.15]\nfor lr in learning_rate:\n    tuning_model = gbm.LGBMClassifier(learning_rate = lr, random_state=1, use_label_encoder = False)\n    scoring = np.mean(cross_validation(tuning_model, X, y_cross, 3))\n    print(str(lr)+str(scoring))'''","metadata":{"execution":{"iopub.status.busy":"2022-07-18T10:17:02.551991Z","iopub.execute_input":"2022-07-18T10:17:02.552772Z","iopub.status.idle":"2022-07-18T10:17:02.631167Z","shell.execute_reply.started":"2022-07-18T10:17:02.552726Z","shell.execute_reply":"2022-07-18T10:17:02.630518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance\n## PIMP Algorithm\nWe can decide which features is good for the model and which isn't using PIMP algorithm. This algorithm shuffle the target value in each iteration of predictions to get a distribution of importance (GINI and Mutual Information) which we can see where our real target value lie in the distribution. We are using fast model like RandomForest Classifier for predictions","metadata":{}},{"cell_type":"code","source":"#PIMP Algorithm to choose features\ndef get_feature_importance(X, y, shuffle = True):\n    if shuffle == True:\n        y = y.copy().sample(frac=1)\n    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n    perm_model = model.fit(X, y)\n    \n    imp_df = pd.DataFrame()\n    imp_df['features'] = X.columns\n    imp_df['importance_split'] = perm_model.feature_importances_\n    imp_df['importance_gain'] = mi(X, y)\n    imp_df['training_score'] = perm_model.score(X, y)\n    \n    return imp_df","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:06.822940Z","iopub.execute_input":"2022-07-18T09:54:06.823396Z","iopub.status.idle":"2022-07-18T09:54:06.831166Z","shell.execute_reply.started":"2022-07-18T09:54:06.823362Z","shell.execute_reply":"2022-07-18T09:54:06.830515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating Null Importance\ndef make_null_imp(X, y, runs):\n    null_imp_df = pd.DataFrame()\n\n    import time\n    start = time.time()\n\n    for i in range(runs):\n        #Get current importance\n        imp_df = get_feature_importance(X, y)\n        imp_df['run'] = i+1\n        #Concat current imp_df to null_imp_df\n        null_imp_df = pd.concat([null_imp_df, imp_df], axis = 0)\n        #Display current runtime and time used\n        spent = (time.time() - start) / 60\n        print(f'done with {i+1} of {runs} (spent {spent} min) ')\n        \n    return null_imp_df\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\ndef display_histogram(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['features'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['features'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['features'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['features'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:06.832440Z","iopub.execute_input":"2022-07-18T09:54:06.832710Z","iopub.status.idle":"2022-07-18T09:54:06.852257Z","shell.execute_reply.started":"2022-07-18T09:54:06.832673Z","shell.execute_reply":"2022-07-18T09:54:06.851658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_imp_df = get_feature_importance(X, y, shuffle = False)\nnull_imp_df = make_null_imp(X, y, 50)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:06.853661Z","iopub.execute_input":"2022-07-18T09:54:06.854537Z","iopub.status.idle":"2022-07-18T09:54:06.869613Z","shell.execute_reply.started":"2022-07-18T09:54:06.854485Z","shell.execute_reply":"2022-07-18T09:54:06.868674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_importance(feature):  \n    display_histogram(actual_imp_df, null_imp_df, feature)\n    \nget_importance('Age')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T09:54:06.871328Z","iopub.execute_input":"2022-07-18T09:54:06.872965Z","iopub.status.idle":"2022-07-18T09:54:06.878574Z","shell.execute_reply.started":"2022-07-18T09:54:06.872916Z","shell.execute_reply":"2022-07-18T09:54:06.877917Z"},"trusted":true},"execution_count":null,"outputs":[]}]}